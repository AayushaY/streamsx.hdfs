<?xml version="1.0" encoding="UTF-8" standalone="no" ?>
<srcCode:sourceModel xmlns:srcCode="http://www.ibm.com/xmlns/prod/streams/spl/sourceCode" xmlns:common="http://www.ibm.com/xmlns/prod/streams/spl/common" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">

  <srcCode:sourceFile uri="hdfsexample/HDFSFileSourceSampleLineFormat.spl">
    <srcCode:compilationUnit>
      <srcCode:splDoc>
        <srcCode:description>
          <srcCode:description>*****************************************************************************
Copyright (C) 2014, International Business Machines Corporation
All Rights Reserved
******************************************************************************</srcCode:description>
        </srcCode:description>
      </srcCode:splDoc>
      <srcCode:splNamespace column="11" line="6" name="hdfsexample"/>
      <srcCode:useDirectives>
        <srcCode:useDirective column="5" line="8" namespaceName="com.ibm.streamsx.hdfs" tail="::HDFS2FileSource"/>
      </srcCode:useDirectives>
      <srcCode:definitions>
        <srcCode:compositeDefinition endColumn="1" endLine="65" startColumn="1" startLine="9">
          <srcCode:compositeHead column="1" line="49" name="HDFS2FileSourceSampleLineFormat">
            <srcCode:splDoc>
              <srcCode:description>
                <srcCode:description>The [HDFS2FileSourceSampleLineFormat] demonstrates how you can use the HDFSFileSink operator
to read a file from the Hadoop file system.

To build this application, you need to set up the HADOOP_HOME environment variable as follows:
* export HADOOP_HOME=&lt;Hadoop installation&gt;

Setup:
To run this sample, you will need to copy the input file from the sample's data directory to 
the Hadoop File System:
1. At the command line, go to the sample's data directory.
1. Run the following command to copy the file from the data directory to HDFS:
  * &lt;HADOOP_HOME&gt;/bin/hadoop fs -copyFromLocal LineFile.txt /user/&lt;userId&gt;

To compile and run this sample in Streams Studio:
1. Start Streams Studio
1. In Streams Explorer view, add the "com.ibm.streams.bigdata" toolkit as one of the toolkit locations
1. From the main menu, select File -&gt; Import.
1. Expand InfoSphere Streams Studio
1. Select Sample SPL Application
1. Select this sample from the Import SPL Sample Application dialog
1. Once the sample is imported, wait for the build to finish.  If autobuild is turned off, select resulting project, right click -&gt; Build Project
1. Once the project is built, select the main composite of the sample, right click -&gt; Launch Active Build Config

If compiling and running from command line, follow these steps:
 1. Create a directory. For example, you can create a directory in your home directory.
  * mkdir $HOME/hdfssamples
 1. Copy the samples to this directory.
  * cp -R $STREAMS_INSTALL/toolkits/com.ibm.streams.bigdata/samples/* $HOME/hdfssamples/
 1. Build one of the sample applications. Go to the appropriate subdirectory and run the make. By default, the sample is compiled as a distributed application. If you want to compile the application as a standalone application, run make standalone instead. Run make clean to return the samples back to their original state.
 1. Run the sample application. 
  * To run the HDFS2FileSourceSampleLineFormat sample application in distributed mode, start your IBM InfoSphere Streams instance, then use the streamtool command to submit the .adl files that were generated during the application build. 
   * streamtool submitjob -i &lt;instance_name&gt; output/Distributed/hdfsexample::HDFS2FileSourceSampleLineFormat/hdfsexample.HDFS2FileSourceSampleLineFormat.adl -P hdfsUri="hdfs://&lt;machine_name&gt;:&lt;port&gt;" -P hdfsUser="&lt;user_name&gt;" 
  * To run the HDFS2FileSourceSampleLineFormat sample application in standalone mode, issue the command:
   * ./output/Standalone/hdfsexample::HDFS2FileSourceSampleLineFormat/bin/standalone hdfsUri="hdfs://&lt;machine_name&gt;:&lt;port&gt;" hdfsUser="&lt;user_name&gt;" 

</srcCode:description>
              </srcCode:description>
              <srcCode:annotation name="param" target="hdfsUri">
                <srcCode:description>URI to HDFS.  If unspecified, the operator expects that the HDFS URI is specified as the fs.defaultFS or fs.default.name property in core-site.xml.</srcCode:description>
              </srcCode:annotation>
              <srcCode:annotation name="param" target="hdfsUser">
                <srcCode:description>User to connect to HDFS.  If unspecified the instance owner running the application will be used as the user
to log onto the HDFS. Example value: "streamsadmin"</srcCode:description>
              </srcCode:annotation>
            </srcCode:splDoc>
          </srcCode:compositeHead>
          <srcCode:compositeBody>
            <srcCode:graph>
              <srcCode:operatorInvocation>
                <srcCode:operatorInvocationHead column="38" line="52" operatorName="HDFS2FileSource">
                  <srcCode:outputs>
                    <srcCode:output column="25" index="0" line="52" streamName="LineStream" type="rstring lines"/>
                  </srcCode:outputs>
                </srcCode:operatorInvocationHead>
                <srcCode:operatorInvocationBody>
                  <srcCode:parameters>
                    <srcCode:parameter column="5" line="55" name="file">
                      <srcCode:value expr="&quot;lineFile.txt&quot;"/>
                    </srcCode:parameter>
                  </srcCode:parameters>
                </srcCode:operatorInvocationBody>
              </srcCode:operatorInvocation>
              <srcCode:operatorInvocation>
                <srcCode:operatorInvocationHead column="9" invocationAlias="MySink" line="58" operatorName="FileSink">
                  <srcCode:inputs>
                    <srcCode:input column="27" index="0" line="58">
                      <srcCode:istream column="27" line="58" name="LineStream"/>
                    </srcCode:input>
                  </srcCode:inputs>
                </srcCode:operatorInvocationHead>
                <srcCode:operatorInvocationBody>
                  <srcCode:parameters>
                    <srcCode:parameter column="5" line="61" name="file">
                      <srcCode:value expr="&quot;LineSink.txt&quot;"/>
                    </srcCode:parameter>
                    <srcCode:parameter column="5" line="62" name="flush">
                      <srcCode:value expr="1u"/>
                    </srcCode:parameter>
                  </srcCode:parameters>
                </srcCode:operatorInvocationBody>
              </srcCode:operatorInvocation>
            </srcCode:graph>
          </srcCode:compositeBody>
        </srcCode:compositeDefinition>
      </srcCode:definitions>
    </srcCode:compilationUnit>
  </srcCode:sourceFile>

  <srcCode:sourceFile uri="hdfsexample/HDFS2FileSourceSampleTxtFormat.spl">
    <srcCode:compilationUnit>
      <srcCode:splNamespace column="11" line="5" name="hdfsexample">
        <srcCode:splDoc>
          <srcCode:description>
            <srcCode:description>*****************************************************************************
Copyright (C) 2014, International Business Machines Corporation
All Rights Reserved
******************************************************************************</srcCode:description>
          </srcCode:description>
        </srcCode:splDoc>
      </srcCode:splNamespace>
      <srcCode:useDirectives>
        <srcCode:useDirective column="5" line="7" namespaceName="com.ibm.streamsx.hdfs" tail="::HDFS2FileSource"/>
      </srcCode:useDirectives>
      <srcCode:definitions>
        <srcCode:compositeDefinition endColumn="1" endLine="79" startColumn="1" startLine="9">
          <srcCode:compositeHead column="1" line="52" name="HDFS2FileSourceSampleTxtFormat">
            <srcCode:splDoc>
              <srcCode:description>
                <srcCode:description>The [HDFS2FileSourceSampleTxtFormat] composite demonstrates how to use the HDFSFileSink
operator to read a file from HDFS in "txt" format.

Contrast from the HDFS2FileSource operator, HDFSFileSink operator only supports writing in "line" format.
If your existing application currently uses the HDFS2FileSource operator to read data in as "txt" format,
you will need to modify your application in order to maintain reading data as "txt" format.

To continue reading in "txt" format, you can insert a "Custom" operator after the HDFSFileSink operator
to convert a line as rstring to a tuple.  You can continue processing the tuple as you normally would.

To build this application, you need to set up the HADOOP_HOME environment variable as follows:
* export HADOOP_HOME=&lt;Hadoop installation&gt;

Setup:
To run this sample, you will need to copy the input file from the sample's data directory to 
the Hadoop File System:
1. At the command line, go to the sample's data directory.
1. Run the following command to copy the file from the data directory to HDFS:
  * &lt;HADOOP_HOME&gt;/bin/hadoop fs -copyFromLocal TxtFile.txt /user/&lt;userId&gt;

To compile and run this sample in Streams Studio:
1. Start Streams Studio
1. In Streams Explorer view, add the "com.ibm.streams.bigdata" toolkit as one of the toolkit locations
1. From the main menu, select File -&gt; Import.
1. Expand InfoSphere Streams Studio
1. Select Sample SPL Application
1. Select this sample from the Import SPL Sample Application dialog
1. Once the sample is imported, wait for the build to finish.  If autobuild is turned off, select resulting project, right click -&gt; Build Project
1. Once the project is built, select the main composite of the sample, right click -&gt; Launch Active Build Config

To compile and run this sample at the command line:
  1. Create a directory. For example, you can create a directory in your home directory.
   * mkdir $HOME/hdfssamples
  1. Copy the samples to this directory.
   * cp -R $STREAMS_INSTALL/toolkits/com.ibm.streams.bigdata/samples/* $HOME/hdfssamples/
  1. Build one of the sample applications. Go to the appropriate subdirectory and run the make. By default, the sample is compiled as a distributed application. If you want to compile the application as a standalone application, run make standalone instead. Run make clean to return the samples back to their original state.
  1. Run the sample application.  
   * To run the HDFS2FileSourceSampleTxtFormat sample application in distributed mode, start your IBM InfoSphere Streams instance, then use the streamtool command to submit the .adl files that were generated during the application build. 
    * streamtool submitjob -i &lt;instance_name&gt; output/Distributed/hdfsexample::HDFS2FileSourceSampleTxtFormat/hdfsexample.HDFS2FileSourceSampleTxtFormat.adl
   * To run the HDFS2FileSourceSampleTxtFormat sample application in standalone mode, issue the command:
    * ./output/Standalone/hdfsexample::HDFS2FileSourceSampleTxtFormat/bin/standalone
</srcCode:description>
              </srcCode:description>
            </srcCode:splDoc>
          </srcCode:compositeHead>
          <srcCode:compositeBody>
            <srcCode:types column="2" line="53">
              <srcCode:type endColumn="49" endLine="55" name="PersonSchema" startColumn="3" startLine="54" value="int32 id, rstring fname, rstring lname, int32 age, rstring gender, float32 score, float64 total"/>
            </srcCode:types>
            <srcCode:graph>
              <srcCode:operatorInvocation>
                <srcCode:operatorInvocationHead column="38" line="58" operatorName="HDFS2FileSource">
                  <srcCode:outputs>
                    <srcCode:output column="25" index="0" line="58" streamName="LineStream" type="rstring lines"/>
                  </srcCode:outputs>
                </srcCode:operatorInvocationHead>
                <srcCode:operatorInvocationBody>
                  <srcCode:parameters>
                    <srcCode:parameter column="5" line="61" name="file">
                      <srcCode:value expr="&quot;TxtFile.txt&quot;"/>
                    </srcCode:parameter>
                  </srcCode:parameters>
                </srcCode:operatorInvocationBody>
              </srcCode:operatorInvocation>
              <srcCode:operatorInvocation>
                <srcCode:operatorInvocationHead column="37" line="65" operatorName="Custom">
                  <srcCode:outputs>
                    <srcCode:output column="24" index="0" line="65" streamName="tupleValue" type="PersonSchema"/>
                  </srcCode:outputs>
                  <srcCode:inputs>
                    <srcCode:input column="44" index="0" line="65">
                      <srcCode:istream column="44" line="65" name="LineStream"/>
                    </srcCode:input>
                  </srcCode:inputs>
                </srcCode:operatorInvocationHead>
                <srcCode:operatorInvocationBody>
                  <srcCode:logic hasState="false">
                    <srcCode:onTuple column="13" line="68" portName="LineStream"/>
                  </srcCode:logic>
                </srcCode:operatorInvocationBody>
              </srcCode:operatorInvocation>
              <srcCode:operatorInvocation>
                <srcCode:operatorInvocationHead column="9" invocationAlias="textSink1" line="73" operatorName="FileSink">
                  <srcCode:inputs>
                    <srcCode:input column="30" index="0" line="73">
                      <srcCode:istream column="30" line="73" name="tupleValue"/>
                    </srcCode:input>
                  </srcCode:inputs>
                </srcCode:operatorInvocationHead>
                <srcCode:operatorInvocationBody>
                  <srcCode:parameters>
                    <srcCode:parameter column="5" line="76" name="file">
                      <srcCode:value expr="&quot;TxtSink.txt&quot;"/>
                    </srcCode:parameter>
                  </srcCode:parameters>
                </srcCode:operatorInvocationBody>
              </srcCode:operatorInvocation>
            </srcCode:graph>
          </srcCode:compositeBody>
        </srcCode:compositeDefinition>
      </srcCode:definitions>
    </srcCode:compilationUnit>
  </srcCode:sourceFile>

</srcCode:sourceModel>
