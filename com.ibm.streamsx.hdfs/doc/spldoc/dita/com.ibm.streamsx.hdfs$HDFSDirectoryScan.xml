<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE reference PUBLIC "-//OASIS//DTD DITA Reference//ENs"
"/homes/hny7/chanskw/streams/ext/dita-ot/dtd/technicalContent/dtd/reference.dtd">
<reference id ="spldoc_operator__com.ibm.streamsx.hdfs__HDFSDirectoryScan">
<title>com.ibm.streamsx.hdfs::HDFSDirectoryScan</title>
<refbody>
<section>
<p>
<xref href="toolkit.xml">
com.ibm.streamsx.hdfs
</xref>
-&gt; <xref href="com.ibm.streamsx.hdfs$.xml">com.ibm.streamsx.hdfs</xref>-&gt; HDFSDirectoryScan</p>
</section>
<section>

<p>The HDFSDirectoryScan operator scans a Hadoop Distributed File System directory for new or modified files. 
</p>
<p>The HDFSDirectoryScan is similar to the DirectoryScan operator. The HDFSDirectoryScan operator repeatedly scans an HDFS directory and writes the names of new or modified files that are found in the directory to the output port. The operator sleeps between scans. 
</p>
<p><b>Exceptions</b>  
</p>
<p>The operator terminates in the following cases: 
</p>
<ul>
<li> The operator cannot connect to HDFS. </li>
<li> The <b>strictMode</b> parameter is true but the directory is not found. </li>
<li> The path that is given by the directory name exists, but is an ordinary file and not a directory. </li>
<li> HDFS failed to give a list of files in the directory. </li>
<li> The pattern that is specified in the pattern parameter fails to compile. </li>
</ul>
<p> <b>Examples</b> 
</p>
<p>In the following example, the HDFSDirectoryScan operator scans the HDFS directory every two seconds. The operator overrides the value that is specified by the <tt>fs.defaultFS</tt> option in the <tt>core-site.xml</tt>. 
</p>
<p>
<codeblock>
<![CDATA[
(stream<rstring filename> Files) as HDFSDirectoryScan_1 = HDFSDirectoryScan()
{
	param
		directory :"/user/myuser/";
		hdfsUri   :"hdfs://hdfsServer:1883";
		sleepTime : 2.0;
}

]]></codeblock>

</p>
<p> 
</p>
</section>
<section outputclass="splprimop">
<object type="image/svg+xml" data="../image/com.ibm.streamsx.hdfs$HDFSDirectoryScan.svg" width="672" height="112">
<desc>Primitive operator image not displayed. Problem loading file: ../image/com.ibm.streamsx.hdfs$HDFSDirectoryScan.svg
</desc>
</object>
</section>
<section>
<title outputclass="splhead-1">Summary</title>
<dl compact="yes">
 <dlentry>
  <dt>Ports</dt>
  <dd>This operator has 1 input port and 1 output port.</dd>
 </dlentry>
 <dlentry>
  <dt>Windowing</dt>
  <dd>This operator does not accept any windowing configurations.</dd>
 </dlentry>
 <dlentry>
  <dt>Parameters</dt>
  <dd>This operator supports 11 parameters. (<tt>directory, initDelay, sleepTime, pattern, strictMode, hdfsUri, hdfsUser, authPrincipal, authKeytab, credFile, configPath</tt>)
</dd>
 </dlentry>
 <dlentry>
  <dt>Metrics</dt>
  <dd>This operator reports  1 metrics.</dd>
 </dlentry>
</dl></section>
<section>
<title outputclass="splhead-1">Properties</title>
<dl compact="yes">
 <dlentry>
  <dt>Implementation</dt>
  <dd>Java</dd>
 </dlentry>
</dl>
</section>
<section>
<p outputclass="splhead-1"><b><xref href="OperatorModel.xml#spldoc_reference_operator_model/input_port_set">Input Ports</xref></b></p><dl>
  <dlentry>
   <dt>Ports (0)</dt>
   <dd>

<p>The HDFSDirectoryScan operator has an optional control input port. You can use this port to change the directory that the operator scans at run time without restarting or recompiling the application.  The expected schema for the input port is of tuple&lt;rstring directory&gt;, a schema containing a single attribute of type rstring.  If a directory scan is in progress when a tuple is received, the scan completes and a new scan starts immediately after and uses the new directory that was specified.  If the operator is sleeping, the operator starts scanning the new directory immediately after it receives an input tuple.
</p>   </dd>
  </dlentry>
    <dlentry>
      <dt>Windowing</dt>
      <dd>

<p>This operator does not support any window configurations.
</p>      </dd>
    </dlentry>
    <dlentry>
      <dt>Properties</dt>
      <dd>
   <sl>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/input_port_set__optional">Optional</xref>: true
     </sli>
   </sl>
   <sl>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/input_port_set__controlport">ControlPort</xref>: true
</sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/input_port_set__windowingmode">WindowingMode</xref>: NonWindowed
</sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/input_port_set__windowpunctuationinputmode">WindowPunctuationInputMode</xref>: Oblivious
</sli>
   </sl>
      </dd>
    </dlentry>
</dl>
</section>
<section>
<p outputclass="splhead-1"><b><xref href="OperatorModel.xml#spldoc_reference_operator_model/output_port_set">Output Ports</xref></b></p><dl>
  <dlentry>
     <dt>Assignments</dt>
       <dd>Java operators do not support output assignments.
       </dd>
  </dlentry>
</dl>
<dl>
  <dlentry>
   <dt>Ports (0)</dt>
   <dd>

<p>The HDFSDirectoryScan operator has one output port. This port provides tuples of type rstring that are encoded in UTF-8 and represent the file names that are found in the directory, one file name per tuple.  The file names do not occur in any particular order. The port is non-mutating and punctuation free.
</p>
<p></p>
   <dl>
    <dlentry>
      <dt>Properties</dt>
      <dd>
   <sl>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/output_port_set__optional">Optional</xref>: false
     </sli>
   </sl>
   <sl>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/output_port_set__windowpunctuationoutputmode">WindowPunctuationOutputMode</xref>: Free
     </sli>
   </sl>
      </dd>
    </dlentry>
   </dl>
   <p></p>
   </dd>
  </dlentry>
</dl>
</section>
<section>
<p outputclass="splhead-1"><b><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters">Parameters</xref></b></p>
<dl>
<dlentry>
<dt id="parameter_directory"><tt>directory</tt></dt>
<dd>
<p>This optional parameter specifies the name of the directory to be scanned. If the name starts with a slash, it is considered an absolute directory that you want to scan. If it does not start with a slash, it is considered a relative directory, relative to the '/user/<i>userid</i>/ directory. This parameter is mandatory if the input port is not specified.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_initDelay"><tt>initDelay</tt></dt>
<dd>
<p>This optional parameter specifies the time to wait in seconds before the operator scans the directory for the first time. The default value is 0.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: float64
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_sleepTime"><tt>sleepTime</tt></dt>
<dd>
<p>This optional parameter specifies the minimum time between directory scans. The default value is 5.0 seconds. 
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: float64
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_pattern"><tt>pattern</tt></dt>
<dd>
<p>This optional parameter limits the file names that are listed to the names that match the specified regular expression. The HDFSDirectoryScan operator ignores file names that do not match the specified regular expression. 
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_strictMode"><tt>strictMode</tt></dt>
<dd>
<p>This optional parameter determines whether the operator reports an error if the directory to be scanned does not exist. If you set this parameter to true and the specified directory does not exist or there is a problem accessing the directory, the operator reports an error and terminates. If you set this parameter to false and the specified directory does not exist or there is a problem accessing the directory, the operator treats it as an empty directory and does not report an error. 
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: boolean
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_hdfsUri"><tt>hdfsUri</tt></dt>
<dd>
<p>This optional parameter of type rstring specifies the uniform resource identifier (URI) that you can use to connect to HDFS. The URI defines the host and the port that you can use to connect to HDFS. The URI has the following format: 
</p>
<ul>
<li> To access HDFS locally or remotely, use <tt>hdfs://hdfshost:hdfsport</tt>  </li>
<li> To access GPFS locally, use <tt>gpfs:///</tt>  </li>
<li> To access GPFS remotely, use <tt>webhdfs://hdfshost:webhdfsport</tt>    </li>
</ul>
<p>If this parameter is not specified, the operator finds the HDFS URI from the <tt>fs.defaultFS</tt> or <tt>fs.default.name</tt> option in the <tt>core-site.xml</tt> HDFS configuration file. The <tt>core-site.xml</tt> is expected to be located in one of the following locations: 
</p>
<ul>
<li> <tt>$HADOOP_HOME/../hadoop-conf</tt> </li>
<li> <tt>$HADOOP_HOME/etc/hadoop</tt>  </li>
<li> <tt>$HADOOP_HOME/conf</tt>  </li>
<li> <tt>$HADOOP_HOME/share/hadoop/hdfs/*</tt>  </li>
<li> <tt>$HADOOP_HOME/share/hadoop/common/*</tt>  </li>
<li> <tt>$HADOOP_HOME/share/hadoop/common/lib/*</tt>  </li>
<li> <tt>$HADOOP_HOME/lib/*</tt>  </li>
<li> <tt>$HADOOP_HOME/*</tt>  </li>
</ul>
<p>You can use the <b>hdfsUri</b> parameter to override the value that is specified for the <tt>fs.defaultFS</tt> or <tt>fs.default.name</tt> option in the <tt>core-site.xml</tt> configuration file.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_hdfsUser"><tt>hdfsUser</tt></dt>
<dd>
<p>This optional parameter specifies the user ID to use when you connect to HDFS. If this parameter is not specified, the operator uses the instance owner ID to connect to HDFS.  
</p>
<p>When you use Kerberos authentication, the operator authenticates with the Hadoop file system as the instance owner by using the values specified by the <b>authPrincipal</b> and <b>authKeytab</b> parameters. After successful authentication, the operator uses the user ID that is specified by the <b>hdfsUser</b> parameter to perform all other operations on the file system. 
</p>
<p>Note: When using Kerberos authentication, to perform operations as the user specified by the <b>hdfsUser</b> parameter, the InfoSphere Streams instance owner must have super user privileges on HDFS or GPFS. 
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_authPrincipal"><tt>authPrincipal</tt></dt>
<dd>
<p>This optional parameter specifies the Kerberos principal that you use for authentication. This value is set to the principal that is created for the InfoSphere Streams instance owner. You must specify this parameter if you want to use Kerberos authentication.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_authKeytab"><tt>authKeytab</tt></dt>
<dd>
<p>This parameter specifies the file that contains the encrypted keys for the user that is specified by the <b>authPrincipal</b> parameter. The operator uses this keytab file to authenticate the user. The keytab file is generated by the administrator. You must specify this parameter to use Kerberos authentication.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_credFile"><tt>credFile</tt></dt>
<dd>
<p>This optional parameter contains the login credentials to use when you connect to GPFS remotely by using the <tt>webhdfs://hdfshost:webhdfsport</tt> schema. The credentials file must contain information on how to authenticate with IBM InfoSphere BigInsights when using the webhdfs schema. For example, the file must contain the user name and password for an IBM InfoSphere BigInsights user.
</p><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
<dlentry>
<dt id="parameter_configPath"><tt>configPath</tt></dt>
<dd>
<p>This optional parameter specifies the absolute path to the directory that contains the HDFS configuration files. If this parameter is not specified, by default the operator looks for the <tt>core-site.xml</tt> file in the following locations: 
</p>
<ul>
<li> <tt>$HADOOP_HOME/../hadoop-conf</tt> </li>
<li> <tt>$HADOOP_HOME/etc/hadoop</tt> </li>
<li> <tt>$HADOOP_HOME/conf</tt> </li>
<li> <tt>$HADOOP_HOME/share/hadoop/hdfs/*</tt> </li>
<li> <tt>$HADOOP_HOME/share/hadoop/common/*</tt> </li>
<li> <tt>$HADOOP_HOME/share/hadoop/common/lib/*</tt> </li>
<li> <tt>$HADOOP_HOME/lib/*</tt> </li>
<li> <tt>$HADOOP_HOME/*</tt> </li>
</ul><dl>
<dlentry>
 <dt>Properties </dt>
 <dd>
  <sl>
  <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__type">Type</xref>: rstring
  </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__cardinality">Cardinality</xref>: 1
     </sli>
     <sli><xref href="OperatorModel.xml#spldoc_reference_operator_model/parameters__parameter__optional">Optional</xref>: true
     </sli>
  </sl>
 </dd>
</dlentry>
</dl>
</dd>
</dlentry>
</dl>
</section>
<section>
<p outputclass="splhead-1"><b><xref href="OperatorModel.xml#spldoc_reference_operator_model/context__codetemplate">Code Templates</xref></b></p>  <dl>
    <dlentry>
     <dt>HDFSDirectoryScan</dt>
     <dd>
       <codeblock>
<![CDATA[stream<rstring name> ${outputStream} = HDFSDirectoryScan()   {
            param
                directory: "${directoryToScan}";
        } ]]>
      </codeblock>
     <p></p>
     </dd>
    </dlentry>
    <dlentry>
     <dt>HDFSDirectoryScan with hdfsUser and hdfsUri</dt>
     <dd>
       <codeblock>
<![CDATA[stream<rstring name> ${outputStream} = HDFSDirectoryScan()   {
            param
                directory: "${directoryToScan}"
                hdfsUser: "${hdfsUser}";
                hdfsUri: "${hdfsUri}";
        } ]]>
      </codeblock>
     <p></p>
     </dd>
    </dlentry>
    <dlentry>
     <dt>HDFSDirectoryScan with HDFSFileSource</dt>
     <dd>
       <codeblock>
<![CDATA[stream<rstring name> ${outputStream} = HDFSDirectoryScan()   {
            param
                directory: "${directoryToScan}";
        }
        
        stream<${schema}> ${fileSourceStream} = HDFSFileSource(${outputStream})   {
        }]]>
      </codeblock>
     <p></p>
     </dd>
    </dlentry>
  </dl>
</section>
<section>
<p outputclass="splhead-1"><b><xref href="OperatorModel.xml#spldoc_reference_operator_model/context__metrics">Metrics</xref></b></p><dl>
  <dlentry>
    <dt><tt>nScans</tt> - <xref href="OperatorModel.xml#spldoc_reference_operator_model/context__metrics_kind_counter">Counter</xref></dt>
    <dd>

<p>The number of times the operator scanned the directory.
</p>
     <p></p>
     </dd>
  </dlentry>
</dl>
</section>
<section>
<p outputclass="splhead-1"><b><xref href="OperatorModel.xml#spldoc_reference_operator_model/context__dependencies">Libraries</xref></b></p>
 <dl>
 <dlentry>
  <dt>Java operator class library
  </dt>
  <dd/>
  <dd><xref href="OperatorModel.xml#spldoc_reference_operator_model/context__dependencies__managedlibrary__libpath">Library Path</xref>: <tt>../../impl/lib/BigData.jar, ../../impl/java/bin</tt></dd>
 </dlentry>
 <dlentry>
  <dt>apache library
  </dt>
  <dd/>
  <dd><xref href="OperatorModel.xml#spldoc_reference_operator_model/context__dependencies__managedlibrary__libpath">Library Path</xref>: <tt>@HADOOP_HOME@/../hadoop-conf, @HADOOP_HOME@/etc/hadoop, @HADOOP_HOME@/conf, @HADOOP_HOME@/share/hadoop/hdfs/*, @HADOOP_HOME@/share/hadoop/common/*, @HADOOP_HOME@/share/hadoop/common/lib/*, @HADOOP_HOME@/lib/*, @HADOOP_HOME@/client/*, @HADOOP_HOME@/*, @HADOOP_HOME@/../hadoop-hdfs/*</tt></dd>
 </dlentry>
 </dl>
</section>
</refbody>
</reference>

