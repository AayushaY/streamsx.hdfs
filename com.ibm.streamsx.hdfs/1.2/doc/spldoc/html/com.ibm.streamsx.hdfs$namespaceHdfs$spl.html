<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xml:lang="en-us" lang="en-us">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta name="copyright" content="(C) Copyright 2005"/>
<meta name="DC.rights.owner" content="(C) Copyright 2005"/>
<meta name="DC.Type" content="reference"/>
<meta name="DC.Title" content="namespaceHdfs.spl"/>
<meta name="DC.Format" content="XHTML"/>
<meta name="DC.Identifier" content="spldoc_compilationunit__"/>
<link rel="stylesheet" type="text/css" href="../html/commonltr.css"/>
<link rel="stylesheet" type="text/css" href="../html/spldoc.css"/>
<title>namespaceHdfs.spl</title>
</head>
<body id="spldoc_compilationunit__">


<h1 class="title topictitle1">namespaceHdfs.spl</h1>

<div class="body refbody">
<div class="section">
<p class="p">
<a class="xref" href="toolkit.html" title="com.ibm.streamsx.hdfs">
com.ibm.streamsx.hdfs
</a>
-&gt; <a class="xref" href="com.ibm.streamsx.hdfs$.html">com.ibm.streamsx.hdfs</a>-&gt; namespaceHdfs.spl</p>

</div>

<div class="section">

<p class="p"><strong class="ph b">Developing and running applications that use the operators in the Big Data HDFS Toolkit</strong> 
</p>

<p class="p">To create applications that use the HDFS Toolkit operators, you must set environment variables and configure either IBM InfoSphere Streams or the SPL compiler to be aware of the location of the toolkit.  
</p>

<p class="p"><strong class="ph b">Before you begin</strong> 
</p>

<ul class="ul">
<li class="li"> Install IBM InfoSphere Streams and set the STREAMS_INSTALL environment variable to the InfoSphere Streams installation directory.  For example: <tt class="ph tt">export STREAMS_INSTALL=Streams-Install-Directory</tt> </li>

<li class="li"> Install a supported version of Hadoop. </li>

<li class="li"> Ensure that InfoSphere Streams has access to Hadoop libraries and configuration files to allow streams processing applications to read and write to HDFS. </li>

</ul>

<p class="p"> <strong class="ph b">Scenario 1</strong> 
</p>

<p class="p">If InfoSphere Streams has access to the location where Hadoop is installed, you only need to set the following environment variables: 
</p>

<ul class="ul">
<li class="li"> For Apache HDFS, Cloudera (CDH4), or Hortonworks (HDP2): 
<ul class="ul">
<li class="li"> Set <strong class="ph b">HADOOP_HOME</strong> to <em class="ph i">Hadoop_Install_Directory</em>. For example, <tt class="ph tt">/usr/lib/hadoop</tt>  </li>

<li class="li"> Set <strong class="ph b">JAVA_HOME</strong> to the location where Java is installed.  </li>

</ul>
</li>

<li class="li"> For IBM InfoSphere BigInsights: 
<ul class="ul">
<li class="li"> Set <strong class="ph b">BIGINSIGHTS_HOME</strong> to <em class="ph i">BigInsights_Install_Directory</em>. For example, <tt class="ph tt">/opt/ibm/biginsights</tt>  </li>

<li class="li"> Set <strong class="ph b">HADOOP_HOME</strong> to <em class="ph i">BigInsights_Install_Directory</em>/IHC. For example, <tt class="ph tt">/opt/ibm/biginsights/IHC</tt>  </li>

<li class="li"> Set <strong class="ph b">JAVA_HOME</strong> to the location where Java is installed.  </li>

</ul>
</li>

</ul>

<p class="p"> <strong class="ph b">Scenario 2</strong>  
</p>

<p class="p">If InfoSphere Streams does not have access to the location where Hadoop is installed: 
</p>

<ol class="ol">
<li class="li"> Copy the Hadoop library files to a location that is accessible to InfoSphere Streams and set the appropriate environment variables. For example: 
<ul class="ul">
<li class="li"> For Apache HDFS, Cloudera (CDH4), or Hortonworks Data Platform (HDP): 
<ol class="ol" type="a">
<li class="li"> Copy <tt class="ph tt">/usr/lib/hadoop</tt> to the InfoSphere Streams cluster and place it in a directory on the cluster, which is accessible to InfoSphere Streams. </li>

<li class="li"> Copy <tt class="ph tt">/usr/lib/hadoop-hdfs</tt> to the InfoSphere Streams cluster and place it in a directory on the cluster, which is accessible to InfoSphere Streams. Note: When copying the directories, you must ensure that symbolic links are dereferenced, otherwise the directory containing the <tt class="ph tt">core-site.xml</tt> file might not get copied to the InfoSphere Streams cluster. On Linux, you can dereference a symbolic link by using the <strong class="ph b">-L</strong> flag. For example, <tt class="ph tt">cp -Lr /usr/lib/hadoop /usr/lib/hadoop-hdfs/path-on-cluster</tt> </li>

</ol>
</li>

<li class="li"> For IBM InfoSphere BigInsights: 
<ol class="ol" type="a">
<li class="li"> Copy <em class="ph i">BigInsights_Install_Directory</em>/IHC to the InfoSphere Streams cluster and place it under a directory on the cluster, which is accessible to InfoSphere Streams.  For example, <tt class="ph tt">/home/Streams/BigInsights_Install_Directory/IHC</tt>. </li>

<li class="li"> Copy <em class="ph i">BigInsights_Install_Directory</em>/hadoop-conf directory to the Streams host and place it under a directory on the cluster, which is accessible to InfoSphere Streams. For example, <tt class="ph tt">/home/Streams/BigInsights_Install_Directory/hadoop-conf</tt> </li>

</ol>
</li>

<li class="li"> For IBM InfoSphere BigInsights installed on GPFS: (Important: If IBM InfoSphere BigInsights is installed on GPFS, you do not need to install InfoSphere Streams on an IBM InfoSphere BigInsights data node. Use the <tt class="ph tt">webhdfs://hdfshost:webhdfsport</tt> schema in the URI that you use to connect to GPFS.)  
<ol class="ol" type="a">
<li class="li"> Copy <em class="ph i">BigInsights_Install_Directory</em>/IHC to InfoSphere Streams cluster and place it under a directory on the cluster, which is accessible to InfoSphere Streams. For example, <tt class="ph tt">/home/Streams/BigInsights_Install_Directory/IHC</tt> 
<ol class="ol" type="i">
<li class="li"> Copy <em class="ph i">BigInsights_Install_Directory</em>/hadoop-conf directory to Streams host and place it under a directory on the cluster, which is accessible to InfoSphere Streams.  For example, <tt class="ph tt">/home/Streams/BigInsights_Install_Directory/hadoop-conf</tt> </li>

<li class="li"> Copy <em class="ph i">BigInsights_Install_Directory</em>/lib/biginsights-gpfs.jar to Streams host and place it under a directory on the cluster, which is accessible to InfoSphere Streams. For example, <tt class="ph tt">/home/Streams/BigInsights_Install_Directory</tt> </li>

</ol>
</li>

</ol>
</li>

</ul>
</li>

<li class="li"> After Hadoop and IBM InfoSphere BigInsights libraries are copied to location that is accessible to InfoSphere Streams, set the following environment variables: 
<ul class="ul">
<li class="li"> For Apache HDFS or Cloudera (CDH4): 
<ul class="ul">
<li class="li"> Set <strong class="ph b">HADOOP_HOME</strong> to <tt class="ph tt">/home/Streams/hadoop</tt>  </li>

<li class="li"> Set <strong class="ph b">JAVA_HOME</strong> to the location where Java is installed  </li>

</ul>
</li>

<li class="li"> For IBM InfoSphere BigInsights: 
<ul class="ul">
<li class="li"> Set <strong class="ph b">HADOOP_HOME</strong> to <tt class="ph tt">/home/Streams/biginsights/IHC</tt> </li>

<li class="li"> Set <strong class="ph b">BIGINSIGHTS_HOME</strong> to <tt class="ph tt">/home/Streams/biginsights</tt>  </li>

<li class="li"> Set <strong class="ph b">JAVA_HOME</strong> to the location where Java is installed.  </li>

</ul>
</li>

<li class="li"> For IBM InfoSphere BigInsights installed on GPFS: 
<ul class="ul">
<li class="li"> Set <strong class="ph b">HADOOP_HOME</strong> to <tt class="ph tt">/opt/ibm/biginsights/IHC/</tt>  
<ul class="ul">
<li class="li"> Set <strong class="ph b">BIGINSIGHTS_HOME</strong> to <tt class="ph tt">/opt/ibm/biginsights</tt>  </li>

<li class="li"> Set <strong class="ph b">JAVA_HOME</strong> to the location where Java is installed.  Important: If you set <strong class="ph b">JAVA_HOME</strong> to the location where IBM Java SDK is installed, you must set the <strong class="ph b">IBM_JAVA_OPTIONS</strong> environment variable by using the <tt class="ph tt">export IBM_JAVA_OPTIONS=-Xrs</tt> command. If you do not set the <strong class="ph b">IBM_JAVA_OPTIONS</strong> environment variable, the operators terminate unexpectedly.  </li>

</ul>
</li>

</ul>
</li>

</ul>
</li>

</ol>

<p class="p"> <strong class="ph b">Procedure</strong> 
</p>

<ol class="ol">
<li class="li"> Verify that the appropriate environment variables are set for the Hadoop distribution that you want to use.  </li>

<li class="li"> Develop your application.  InfoSphere Streams Studio can help you create and debug SPL and SPL mixed-mode applications. To use the operators from the HDFS Toolkit, you must add the toolkit location. To avoid the need to fully qualify the operators, add a use directive in your application. For example, add the following clause in your SPL source file: <tt class="ph tt">use com.ibm.streamsx.hdfs::*;</tt> You can also specify a use directive for individual operators by replacing the asterisk (*) with the operator name. For example: `use use com.ibm.streams.streamsx.hdfs::HDFSFileSource;` </li>

<li class="li"> If IBM InfoSphere BigInsights is installed on GPFS:  
<ul class="ul">
<li class="li"> To access GPFS locally, set the <tt class="ph tt">fs.defaultFS</tt> option in the <tt class="ph tt">core-site.xml</tt> configuration file to <tt class="ph tt">gpfs:///</tt>.  </li>

<li class="li"> To access GPFS remotely, modify the <tt class="ph tt">core-site.xml</tt> that you have copied over from the remote system. Set the <tt class="ph tt">fs.default.FS</tt> option in the <tt class="ph tt">core-site.xml</tt> configuration file to <tt class="ph tt">webhdfs://hdfshost:webhdfsport</tt>. For example, <tt class="ph tt">webhdfs://myhdfshost:14000</tt>. Ensure that the user is set up to access the file system by using the webhdfs schema.  </li>

</ul>
</li>

<li class="li"> To read and write to HDFS, specify a uniform resource identifier (URI) to connect to HDFS. You can specify the URI in one of the following ways: 
<ul class="ul">
<li class="li"> Specify a value for the <tt class="ph tt">fs.defautlFS</tt> or <tt class="ph tt">fs.default.name</tt> option in the <tt class="ph tt">core-site.xml</tt> HDFS configuration file. By default, the operators look for the <tt class="ph tt">core-site.xml</tt> file in the following directories: 
<ul class="ul">
<li class="li"> <tt class="ph tt">$HADOOP_HOME/../hadoop-conf</tt>  </li>

<li class="li"> <tt class="ph tt">$HADOOP_HOME/etc/hadoop</tt>  </li>

<li class="li"> <tt class="ph tt">$HADOOP_HOME/conf</tt>  </li>

<li class="li"> <tt class="ph tt">$HADOOP_HOME/share/hadoop/hdfs/*</tt>  </li>

<li class="li"> <tt class="ph tt">$HADOOP_HOME/share/hadoop/common/*</tt>  </li>

<li class="li"> <tt class="ph tt">$HADOOP_HOME/share/hadoop/common/lib/*</tt>  </li>

<li class="li"> <tt class="ph tt">$HADOOP_HOME/lib/*</tt>  </li>

<li class="li"> <tt class="ph tt">$HADOOP_HOME/*</tt> </li>

</ul>
</li>

</ul>
Tip: To specify a different location for the HDFS configuration files, set the <strong class="ph b">configPath</strong> operator parameter. 
<ul class="ul">
<li class="li"> Specify a value for the hdfsUri operator parameter.  </li>

</ul>
</li>

<li class="li"> Build your application.  You can use the <strong class="ph b">sc</strong> command or Streams Studio. To build the application in Streams Studio, you must add the toolkit location if you did not already do so. To build the application from the command line, you must configure the SPL compiler to find the root directory of the toolkit. Use one of the following methods: 
<ul class="ul">
<li class="li"> Set the <strong class="ph b">STREAMS_SPLPATH</strong> environment variable to the root directory of a toolkit or multiple toolkits (using a colon (:) as a separator). For example: </li>

</ul>
<tt class="ph tt">export STREAMS_SPLPATH=$STREAMS_INSTALL/toolkits/com.ibm.streams.bigdata</tt> 
<ul class="ul">
<li class="li"> Specify the <strong class="ph b">-t</strong> or <strong class="ph b">--spl-path</strong> command parameter when you run the <strong class="ph b">sc</strong> command. For example: </li>

</ul>
<tt class="ph tt">sc -t $STREAMS_INSTALL/toolkits/com.ibm.streams.bigdata -M MyMain</tt> Note: These command parameters override the <strong class="ph b">STREAMS_SPLPATH</strong> environment variable. </li>

<li class="li"> Start the InfoSphere Streams instance.  </li>

<li class="li"> Run the application. You can submit the application as a job by using the <strong class="ph b">streamtool submitjob</strong> command or by using Streams Studio.  </li>

</ol>

</div>

</div>


</body>
</html>